{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to do a very simplified version of the no-free lunch theorem. Assume that the input data X = {x1, x2, …, x1000}are some set of 1000 integers distict integers (no two are the same). Now assume that we will use the last 80% as training data, so that Xtrain = {x201, …, x1000}and the first 200 data points are for our test set. We also assume that each data point has associated to it a “label”, so that Y = {y1, …, yn}and for every yk, yk ∈ { − 1, 1}, in other words all the yk labels are +1 or -1. Because no two inputs inX are the same, we say that this is a “noiseless” case, so there is a “perfect classifier” function f : X → Y, which takes as input xk and outputs the correct label f(xk) = yk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose all the labels are +1 so that f(xk) = 1 = yk. Now suppose, by some algorithm we create a classifier g which is +1 for even indices g(x2k) = 1but -1 for odd indices g(x2k − 1) =  − 1. What is the accuracy of g on the test set Xtest (the first 200 data points)? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can expect the accuracy to be 50% because this is dummy, random classifier assigning a binary based on the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we imagine we may get new input data. Perhaps our xk are the same but yk are different and given by the “perfect” labeling function f. If we change even one of the yk, that would be a different labeling function f. So taken together, choosing each and every of the labels yk is the same as picking a labeling function f. So we can think about the set of all possible labeling functions. For example one f is always 1, as in the previous problem. Another f, is alway − 1. Another has f is +1 for xk if and only if k is prime, and -1 otherwise. And, many more. With 1000 data points, how many possible true functions f are there? (why)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are only TWO true functions, because the function is based on a single variable i.e. the index (https://cs.fit.edu/~wds/classes/adm/Lectures/BooleanFunctions.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don’t make any assumptions then f could be chosen at random, from the set of all possible labeling functions f. If we compute the accuracy of the g we chose above, on every one of these f, and averaged over all of them, what would the accuracy of g be on the test set be and why? In otherwords what is the expected accuracy of g over all possible labelings on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can still expect the classifier to be 50% accurate because in essence it all maps to a dummy, random classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we didn’t really use the training set for anything. Again if f is chosen at random, and now g is the output of some algorithm that looks at the last 80% of the data, but we test on the first 20% (hold out set), what is the expected accuracy over all possible f? Is it now any different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we run this test over and over we will see that the average accuracy is still 50%. For a small training set it may be difficult to approach the true accuracy at first, but if we run the model multiple times statistically we will approach the true value of 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
